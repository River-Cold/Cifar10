# Cifar10 Classification

该仓库包括以下文件：

- 预训练模型
  - LeNet5基础网络训练的模型model100.pkl
  - 增加学习率衰减后训练出的模型model100_lr.pkl
- 数据集
- 参考代码
  - LeNet网络代码
  - 训练和测试代码

## 问题描述

使用Pytorch等深度学习框架实现LeNet网络模型，针对Cifar10图像数据集进行图像分类，在训练集上训练模型，在测试集上测试模型。通过不断调节各种超参数及网络模型的细节，已到达在测试集上有更高的分类准确率的目的。

## 数据集介绍

CIFAR-10数据集包含10个类别的60000个32x32彩色图像，每个类别6000个图像。 有50000张训练图像和10000张测试图像。数据集地址如下：

[CIFAR-10 and CIFAR-100 datasets (toronto.edu)](http://www.cs.toronto.edu/~kriz/cifar.html)

数据集分为五个训练批次和一个测试批次，每个批次有 10000 张图像。 测试批次包含从每个类别中随机选择的 1000 张图像。 训练批次包含随机顺序的剩余图像，但一些训练批次可能包含比另一个类别更多的图像。 在它们之间，训练批次包含来自每个类的 5000 张图像。

‎以下是数据集中的类别，以及每种类别中的 10 张随机图像：

（10种类别分别为飞机、汽车、鸟、猫、鹿、狗、青蛙、马、船、卡车。）

![img](https://pytorch.org/tutorials/_images/cifar10.png)

## 分类模型

### LeNet网络模型简介

LeNet-5出自论文Gradient-Based Learning Applied to Document Recognition，是一种用于手写体字符识别的非常高效的卷积神经网络。

![img](https://cdn.nlark.com/yuque/0/2021/png/21953026/1632214210270-a2756f81-9317-4592-b861-e441ab1e737c.png)

### LeNet网络模型结构

LeNet5 这个网络虽然很小，但是它包含了卷积神经网络的基本模块：卷积层，池化层，全连接层。是其他卷积神经网络模型的基础， 这里我们对LeNet5进行深入分析。

| 层名称           | 输入图片大小 | 输出图片大小 | 卷积核大小 | 输入通道数 | 输出通道数 | 步长 |
| ---------------- | ------------ | ------------ | ---------- | ---------- | ---------- | ---- |
| Input（输入层）  | 32*32        | 32*32        | /          | 1          | 1          | /    |
| C1（卷积层）     | 32*32        | 28*28        | 5*5        | 1          | 6          | 1    |
| S2（池化层）     | 28*28        | 14*14        | 2*2        | 6          | 6          | 2    |
| C3（卷积层）     | 14*14        | 10*10        | 5*5        | 6          | 16         | 1    |
| S4（池化层）     | 10*10        | 5*5          | 2*2        | 16         | 16         | 2    |
| C5（卷积层）     | 5*5          | 1*1          | 5*5        | 16         | 120        | 1    |
| F6（全连接层）   | 1*1          | 1*1          | /          | 120        | 84         | /    |
| Output（输出层） | 1*1          | 1*1          | /          | 120        | 10         | /    |

## 训练模型在测试集上的结果

通过LeNet5训练后的网络在整个测试集上的平均准确率达到59%。

![img](https://cdn.nlark.com/yuque/0/2021/png/21953026/1632361624912-3ea0a71d-db38-40cd-ae10-976bfc0bddd0.png)

## 调整网络超参数对训练出模型准确率的影响

### 超参数的定义

在机器学习的过程中，超参数是指在开始机器学习之前，就人为设置好的参数。模型参数是指通过训练得到的参数数据。
通常情况下，需要对超参数进行优化，给神经网络选择一组最优超参数，以提高学习的性能和效果。

### 常见的超参数

- 学习率learning rate，初值设为0.01。
- 迭代次数epochs，初值设为100。
- 激活函数activation function，采用ReLu激活函数。
- 批量大小batch_size，初值设为128。
- 优化器optimizer，采用Adam优化器，Adam是一种自适应学习率的优化方法。

### 学习率的优化方法

学习率优化：合理的学习率可以使优化器快速收敛，一般在训练初期给予较大的学习率，随着训练的进行，学习率逐渐减小。学习率的调整主要分为自定义调整和自适应调整。本文采用自定义调整的方式进行优化，Pytorch中的具体实现方式如下：

![img](https://cdn.nlark.com/yuque/0/2021/png/21953026/1632366923332-56a559a0-5e2e-4fc1-aab3-35fda41e1ffb.png)

### 优化前后结果对比

| 超参数调整          | 平均准确率 | 提升 |
| ------------------- | ---------- | ---- |
| LeNet5基本神经网络  | 59%        | /    |
| 基础网络+学习率衰减 | 61%        | +2%  |

