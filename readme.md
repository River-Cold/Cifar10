# Cifar10 Classification

该仓库包括以下文件：

- 预训练模型
  - LeNet5基础网络训练模型model10.pkl
  - 调整学习率训练模型model10_lr_refinement.pkl
  - 调整激活函数训练模型model10_af_refinement.pkl
  - 调整迭代次数训练模型model20_af_refinement.pkl
- 数据集
- 参考代码
  - LeNet基础网络代码lenet5.py
  - 训练和测试代码cifar10-LeNet5.ipynb

## 问题描述

使用Pytorch等深度学习框架实现LeNet网络模型，针对Cifar10图像数据集进行图像分类，在训练集上训练模型，在测试集上测试模型。通过不断调节各种超参数及网络模型的细节，已到达在测试集上有更高的分类准确率的目的。

## 数据集介绍

CIFAR-10数据集包含10个类别的60000个32x32彩色图像，每个类别6000个图像。 有50000张训练图像和10000张测试图像。数据集地址如下：

[CIFAR-10 and CIFAR-100 datasets (toronto.edu)](http://www.cs.toronto.edu/~kriz/cifar.html)

数据集分为五个训练批次和一个测试批次，每个批次有 10000 张图像。 测试批次包含从每个类别中随机选择的 1000 张图像。 训练批次包含随机顺序的剩余图像，但一些训练批次可能包含比另一个类别更多的图像。 在它们之间，训练批次包含来自每个类的 5000 张图像。

‎以下是数据集中的类别，以及每种类别中的 10 张随机图像：

（10种类别分别为飞机、汽车、鸟、猫、鹿、狗、青蛙、马、船、卡车。）

![img](https://pytorch.org/tutorials/_images/cifar10.png)

## 分类模型

### LeNet网络模型简介

LeNet-5出自论文Gradient-Based Learning Applied to Document Recognition，是一种用于手写体字符识别的非常高效的卷积神经网络。

![img](https://cdn.nlark.com/yuque/0/2021/png/21953026/1632214210270-a2756f81-9317-4592-b861-e441ab1e737c.png)

### LeNet网络模型结构

LeNet5 这个网络虽然很小，但是它包含了卷积神经网络的基本模块：卷积层，池化层，全连接层。是其他卷积神经网络模型的基础， 这里我们对LeNet5进行深入分析。

| 层名称           | 输入图片大小 | 输出图片大小 | 卷积核大小 | 输入通道数 | 输出通道数 | 步长 |
| ---------------- | ------------ | ------------ | ---------- | ---------- | ---------- | ---- |
| Input（输入层）  | 32*32        | 32*32        | /          | 1          | 1          | /    |
| C1（卷积层）     | 32*32        | 28*28        | 5*5        | 1          | 6          | 1    |
| S2（池化层）     | 28*28        | 14*14        | 2*2        | 6          | 6          | 2    |
| C3（卷积层）     | 14*14        | 10*10        | 5*5        | 6          | 16         | 1    |
| S4（池化层）     | 10*10        | 5*5          | 2*2        | 16         | 16         | 2    |
| C5（卷积层）     | 5*5          | 1*1          | 5*5        | 16         | 120        | 1    |
| F6（全连接层）   | 1*1          | 1*1          | /          | 120        | 84         | /    |
| Output（输出层） | 1*1          | 1*1          | /          | 120        | 10         | /    |

## 训练模型在测试集上的结果

通过LeNet5训练后的网络在整个测试集上的平均准确率达到59%。

![img](https://cdn.nlark.com/yuque/0/2021/png/21953026/1632361624912-3ea0a71d-db38-40cd-ae10-976bfc0bddd0.png)

## 调整网络超参数对训练出模型准确率的影响

### 超参数的定义

在机器学习的过程中，超参数是指在开始机器学习之前，就人为设置好的参数。模型参数是指通过训练得到的参数数据。
通常情况下，需要对超参数进行优化，给神经网络选择一组最优超参数，以提高学习的性能和效果。

### 常见的超参数

- 学习率learning rate，初值设为0.01。
- 迭代次数epochs，初值设为100。
- 激活函数activation function，采用ReLu激活函数。
- 批量大小batch_size，初值设为128。
- 优化器optimizer，采用Adam优化器，Adam是一种自适应学习率的优化方法。

#### 学习率调整

合理的学习率可以使优化器快速收敛，一般在训练初期给予较大的学习率，随着训练的进行，学习率逐渐减小。学习率的调整主要分为自定义调整和自适应调整。本文采用自定义调整的方式进行优化，Pytorch中的具体实现方式如下：

![img](https://cdn.nlark.com/yuque/0/2021/png/21953026/1632366923332-56a559a0-5e2e-4fc1-aab3-35fda41e1ffb.png)

#### 激活函数调整

LeNet基础网络中使用的激活函数是ReLU函数，而ReLU函数存在神经元死亡现象，某些神经元可能永远不会被激活，导致相应参数永远不会被更新（在负数部分，梯度为0）。

LeakyReLU的提出就是为了解决神经元”死亡“问题，LeakyReLU与ReLU很相似，仅在输入小于0的部分有差别，ReLU输入小于0的部分值都为0，而LeakyReLU输入小于0的部分，值为负，且有微小的梯度。

本文中LeNet基础网络使用的激活函数为ReLU函数，调整后的激活函数为LeakyReLU函数。

#### 迭代次数调整

一个epoch指所有的数据送入网络中，完成一次前向计算及反向传播的过程。随着epoch数量的增加，神经网络中权重更新迭代的次数增多，曲线从最开始的不拟合状态，慢慢进入优化拟合状态，最终进入过拟合。

本文设置epoch初始值为10，调整后的epoch值为20。

### 优化前后结果对比

本文的优化步骤为先调整学习率，再调整激活函数，再调整迭代次数。

| 调整超参数 | 调整前 | 调整后 |
| ---------- | ------ | ------ |
| 学习率     | 59%    | 61%    |
| 激活函数   | 61%    | 62%    |
| 迭代次数   | 62%    | 63%    |
